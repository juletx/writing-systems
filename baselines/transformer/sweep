#!/bin/bash
# Sweeps over data and hyperparameters.

set -euo pipefail

# Defaults.
readonly SEED=1917
readonly CRITERION=label_smoothed_cross_entropy
readonly LABEL_SMOOTHING=.1
readonly OPTIMIZER=adam
readonly LR=1e-3
readonly LR_SCHEDULER=inverse_sqrt
readonly WARMUP_INIT_LR=1e-7
readonly WARMUP_UPDATES=1000
readonly CLIP_NORM=1.
readonly MAX_UPDATE=5000
readonly SAVE_INTERVAL=5
readonly ACTIVATION_FN=relu
readonly DATABIN=data-bin
readonly CKPTS=checkpoints
readonly TENSORBOARD=logs

# Encoder embedding dim.
readonly EED=256
# Encoder hidden layer size.
readonly EHS=1024
# Encoder number of layers.
readonly ENL=4
# Encoder number of attention heads.
readonly EAH=4
# Decoder embedding dim.
readonly DED=256
# Decoder hidden layer size.
readonly DHS=1024
# Decoder number of layers.
readonly DNL=4
# Decoder number of attention heads.
readonly DAH=4
# Batch size.
readonly BATCH=400
# Dropout.
readonly DROPOUT=.1

# Prediction options.
readonly BEAM=5

train() {
    local -r CP="$1"; shift
    local -r TASK="$1"; shift
    if [ $TASK == "g2p" ]; then
        INPUT="graphemes"
        OUTPUT="phonemes"
    else
        INPUT="phonemes"
        OUTPUT="graphemes"
    fi
    fairseq-train \
        "${DATABIN}/${TASK}/${LANGUAGE}" \
        --save-dir="${CP}" \
        --source-lang="${LANGUAGE}.${INPUT}" \
        --target-lang="${LANGUAGE}.${OUTPUT}" \
        --seed="${SEED}" \
        --arch=transformer \
        --attention-dropout="${DROPOUT}" \
        --activation-dropout="${DROPOUT}" \
        --activation-fn="${ACTIVATION_FN}" \
        --encoder-embed-dim="${EED}" \
        --encoder-ffn-embed-dim="${EHS}" \
        --encoder-layers="${ENL}" \
        --encoder-attention-heads="${EAH}" \
        --encoder-normalize-before \
        --decoder-embed-dim="${DED}" \
        --decoder-ffn-embed-dim="${DHS}" \
        --decoder-layers="${DNL}" \
        --decoder-attention-heads="${DAH}" \
        --decoder-normalize-before \
        --share-decoder-input-output-embed \
        --criterion="${CRITERION}" \
        --label-smoothing="${LABEL_SMOOTHING}" \
        --optimizer="${OPTIMIZER}" \
        --lr="${LR}" \
        --lr-scheduler="${LR_SCHEDULER}" \
        --warmup-init-lr="${WARMUP_INIT_LR}" \
        --warmup-updates="${WARMUP_UPDATES}" \
        --clip-norm="${CLIP_NORM}" \
        --batch-size="${BATCH}" \
        --max-update="${MAX_UPDATE}" \
        --patience=50 \
        --no-epoch-checkpoints \
        --tensorboard-logdir="${TENSORBOARD}/${TASK}/${LANGUAGE}" \
        "$@"   # In case we need more configuration control.
}

evaluate() {
    local -r CP="$1"; shift
    local -r TASK="$1"; shift
    if [ $TASK == "g2p" ]; then
        INPUT="graphemes"
        OUTPUT="phonemes"
    else
        INPUT="phonemes"
        OUTPUT="graphemes"
    fi
    SET="test"
    CHECKPOINT="${CP}/checkpoint_best.pt"
    RES="${CP}/${SET}.res"
    # Don't overwrite an existing prediction file.
    if [[ -f "${RES}" ]]; then
        continue
    fi
    echo "Evaluating into ${RES}"
    OUT="${CP}/${SET}.out"
    TSV="${CP}/${SET}.tsv"
    # Makes raw predictions.
    fairseq-generate \
        "${DATABIN}/${TASK}/${LANGUAGE}" \
        --source-lang="${LANGUAGE}.${INPUT}" \
        --target-lang="${LANGUAGE}.${OUTPUT}" \
        --path="${CHECKPOINT}" \
        --seed="${SEED}" \
        --gen-subset="${SET}" \
        --beam="${BEAM}" \
        --no-progress-bar \
        > "${OUT}"
    # Extracts the predictions into a TSV file.
    paste \
        <(cat "${OUT}" | grep '^T-' | cut -f2) \
        <(cat "${OUT}" | grep '^H-' | cut -f3) \
        > "${TSV}"
    # Applies the evaluation script to the TSV file.
    ../../evaluation/./evaluate.py "${TSV}" > "${RES}" 2> /dev/null
    # Cleans up intermediate files.
    rm -f "${OUT}"
}

main() {
    for TASK in g2p p2g; do
        for LANGUAGE in $(ls ${DATABIN}/${TASK}); do
            train "${CKPTS}/${TASK}/${LANGUAGE}" "${TASK}"
            evaluate "${CKPTS}/${TASK}/${LANGUAGE}" "${TASK}"
        done
    done
}

main
