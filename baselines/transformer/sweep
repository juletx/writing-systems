#!/bin/bash
# Sweeps over data and hyperparameters.

set -euo pipefail

# Defaults.
readonly SEED=1917
readonly CRITERION=label_smoothed_cross_entropy
readonly LABEL_SMOOTHING=.1
readonly OPTIMIZER=adam
readonly LR=1e-3
readonly LR_SCHEDULER=inverse_sqrt
readonly WARMUP_INIT_LR=1e-7
readonly WARMUP_UPDATES=1000
readonly CLIP_NORM=1.
readonly MAX_UPDATE=5000
readonly SAVE_INTERVAL=5
readonly ENCODER_LAYERS=4
readonly ENCODER_ATTENTION_HEADS=4
readonly DECODER_LAYERS=4
readonly DECODER_ATTENTION_HEADS=4
readonly ACTIVATION_FN=relu

# Hyperparameters to be tuned.
readonly BATCHES=(256 512 1024)
readonly DROPOUTS=(.1 .2 .3)

# Prediction options.
readonly BEAM=5

train() {
    local -r CP="$1"; shift
    fairseq-train \
        "data-bin/${LANGUAGE}" \
        --save-dir="${CP}" \
        --source-lang="${LANGUAGE}.graphemes" \
        --target-lang="${LANGUAGE}.phonemes" \
        --disable-validation \
        --seed="${SEED}" \
        --arch=transformer \
        --attention-dropout="${DROPOUT}" \
        --activation-dropout="${DROPOUT}" \
        --activation-fn="${ACTIVATION_FN}" \
        --encoder-embed-dim="${EED}" \
        --encoder-ffn-embed-dim="${EHS}" \
        --encoder-layers="${ENCODER_LAYERS}" \
        --encoder-attention-heads="${ENCODER_ATTENTION_HEADS}" \
        --encoder-normalize-before \
        --decoder-embed-dim="${DED}" \
        --decoder-ffn-embed-dim="${DHS}" \
        --decoder-layers="${DECODER_LAYERS}" \
        --decoder-attention-heads="${DECODER_ATTENTION_HEADS}" \
        --decoder-normalize-before \
        --share-decoder-input-output-embed \
        --criterion="${CRITERION}" \
        --label-smoothing="${LABEL_SMOOTHING}" \
        --optimizer="${OPTIMIZER}" \
        --lr="${LR}" \
        --lr-scheduler="${LR_SCHEDULER}" \
        --warmup-init-lr="${WARMUP_INIT_LR}" \
        --warmup-updates="${WARMUP_UPDATES}" \
        --clip-norm="${CLIP_NORM}" \
        --batch-size="${BATCH}" \
        --max-update="${MAX_UPDATE}" \
        --save-interval="${SAVE_INTERVAL}" \
        "$@"   # In case we need more configuration control.
}

evaluate() {
    local -r CP="$1"; shift
    local -r MODE="$1"; shift
    # Fairseq insists on calling the dev-set "valid"; hack around this.
    local -r FAIRSEQ_MODE="${MODE/dev/valid}"
    for CHECKPOINT in $(ls ${CP}/checkpoint[1-9]*.pt 2> /dev/null); do
        RES="${CHECKPOINT/.pt/-${MODE}.res}"
        # Don't overwrite an existing prediction file.
        if [[ -f "${RES}" ]]; then
            continue
        fi
        echo "Evaluating into ${RES}"
        OUT="${CP}/${MODE}.out"
        TSV="${CP}/${MODE}.tsv"
        # Makes raw predictions.
        fairseq-generate \
            "data-bin/${LANGUAGE}" \
            --source-lang="${LANGUAGE}.graphemes" \
            --target-lang="${LANGUAGE}.phonemes" \
            --path="${CHECKPOINT}" \
            --seed="${SEED}" \
            --gen-subset="${FAIRSEQ_MODE}" \
            --beam="${BEAM}" \
            --no-progress-bar \
            > "${OUT}"
        # Extracts the predictions into a TSV file.
        paste \
            <(cat "${OUT}" | grep '^T-' | cut -f2) \
            <(cat "${OUT}" | grep '^H-' | cut -f3) \
            > "${TSV}"
        # Applies the evaluation script to the TSV file.
        ../../evaluation/./evaluate.py "${TSV}" > "${RES}" 2> /dev/null
        # Cleans up intermediate files.
        rm -f "${OUT}" "${TSV}"
    done
}

train_evaluate() {
    local -r CP="$1"; shift
    train "${CP}"
    evaluate "${CP}" dev
    evaluate "${CP}" test
}

# These set the encoder and/or decoder size.

small_encoder() {
  export EED=128
  export EHS=512
}

large_encoder() {
  export EED=256
  export EHS=1024
}

small_decoder() {
  export DED=128
  export DHS=512
}

large_decoder() {
  export DED=256
  export DHS=1024
}

main() {
    for LANGUAGE in $(ls data-bin); do
        for BATCH in "${BATCHES[@]}"; do
            for DROPOUT in "${DROPOUTS[@]}"; do
                small_encoder; small_decoder
                train_evaluate "checkpoints/${LANGUAGE}-${BATCH}-${DROPOUT}-s-s"
                small_encoder; large_decoder
                train_evaluate "checkpoints/${LANGUAGE}-${BATCH}-${DROPOUT}-s-l"
                large_encoder; small_decoder
                train_evaluate "checkpoints/${LANGUAGE}-${BATCH}-${DROPOUT}-l-s"
                large_encoder; large_decoder
                train_evaluate "checkpoints/${LANGUAGE}-${BATCH}-${DROPOUT}-l-l"
            done
        done
    done
}

main
