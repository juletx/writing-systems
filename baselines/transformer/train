#!/bin/bash
# Sweeps over data and hyperparameters.

set -euo pipefail

# Defaults.
readonly SEED=1917
readonly CRITERION=label_smoothed_cross_entropy
readonly LABEL_SMOOTHING=.1
readonly OPTIMIZER=adam
readonly LR=1e-3
readonly LR_SCHEDULER=inverse_sqrt
readonly WARMUP_INIT_LR=1e-7
readonly WARMUP_UPDATES=1000
readonly CLIP_NORM=1.
readonly MAX_UPDATE=5000
readonly ACTIVATION_FN=relu
readonly DATABIN=data-bin
readonly CKPTS=checkpoints
readonly TENSORBOARD=logs

# Encoder embedding dim.
readonly EED=256
# Encoder hidden layer size.
readonly EHS=1024
# Encoder number of layers.
readonly ENL=4
# Encoder number of attention heads.
readonly EAH=4
# Decoder embedding dim.
readonly DED=256
# Decoder hidden layer size.
readonly DHS=1024
# Decoder number of layers.
readonly DNL=4
# Decoder number of attention heads.
readonly DAH=4
# Batch size.
readonly BATCH=400
# Dropout.
readonly DROPOUT=.1

train() {
    local -r CP="$1"; shift
    local -r TASK="$1"; shift
    if [ $TASK == "g2p" ]; then
        INPUT="graphemes"
        OUTPUT="phonemes"
    else
        INPUT="phonemes"
        OUTPUT="graphemes"
    fi
    fairseq-train \
        "${DATABIN}/${TASK}/${LANGUAGE}" \
        --save-dir="${CP}" \
        --source-lang="${LANGUAGE}.${INPUT}" \
        --target-lang="${LANGUAGE}.${OUTPUT}" \
        --seed="${SEED}" \
        --arch=transformer \
        --attention-dropout="${DROPOUT}" \
        --activation-dropout="${DROPOUT}" \
        --activation-fn="${ACTIVATION_FN}" \
        --encoder-embed-dim="${EED}" \
        --encoder-ffn-embed-dim="${EHS}" \
        --encoder-layers="${ENL}" \
        --encoder-attention-heads="${EAH}" \
        --encoder-normalize-before \
        --decoder-embed-dim="${DED}" \
        --decoder-ffn-embed-dim="${DHS}" \
        --decoder-layers="${DNL}" \
        --decoder-attention-heads="${DAH}" \
        --decoder-normalize-before \
        --share-decoder-input-output-embed \
        --criterion="${CRITERION}" \
        --label-smoothing="${LABEL_SMOOTHING}" \
        --optimizer="${OPTIMIZER}" \
        --lr="${LR}" \
        --lr-scheduler="${LR_SCHEDULER}" \
        --warmup-init-lr="${WARMUP_INIT_LR}" \
        --warmup-updates="${WARMUP_UPDATES}" \
        --clip-norm="${CLIP_NORM}" \
        --batch-size="${BATCH}" \
        --max-update="${MAX_UPDATE}" \
        --patience=50 \
        --no-epoch-checkpoints \
        --tensorboard-logdir="${TENSORBOARD}/${TASK}/${LANGUAGE}" \
        "$@"   # In case we need more configuration control.
}

main() {
    for TASK in g2p p2g; do
        for LANGUAGE in $(ls ${DATABIN}/${TASK}); do
            train "${CKPTS}/${TASK}/${LANGUAGE}" "${TASK}"
        done
    done
}

main
